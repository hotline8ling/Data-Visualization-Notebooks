{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Document\n",
    "\n",
    "We are going to move from an individual sentence to a document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "This notebook expects that the needed packages have already been installed and that NLTK has been setup correctly via the `setup` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Text\n",
    "\n",
    "First, lets get some text data. \n",
    "\n",
    "Here we use what we've learned about python to read in some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144342"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"alice.txt\"\n",
    "\n",
    "with open(filename) as handle:\n",
    "    text = handle.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginni'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real quick, lets take away newline characters to be able to read the text better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I. Down the Rabbit-Hole  Alice was beginni'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove new line characters\n",
    "text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\rojgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "From our Sentence analysis, we know that the process of splitting up a document into small, word-like _meaningful units_ is known as **tokenization**. \n",
    "\n",
    "We will start with word tokenization and look at multi-word tokens in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33533"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use word tokenizer\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, NLTK's word tokenizer leaves punctuation as separate tokens. We will take care of that in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Searching in a Document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has some search features in their `Text` object. \n",
    "\n",
    "Useful for interactive searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.Text has useful methods for searching\n",
    "my_text = nltk.Text(tokens)\n",
    "\n",
    "# But it initially looks the same as are tokens array\n",
    "my_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`findall`** takes a string with tokens delimited with angle brackets `<token>`. Regular expressions can be used in and around the angle brackets to robustly find token matches.\n",
    "\n",
    "Here we find the preceding and following word for every use of \"Hare\" in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March Hare .; March Hare was; March Hare will; March Hare :; March\n",
      "Hare and; March Hare said; March Hare .; March Hare .; March Hare .;\n",
      "March Hare went; March Hare ,; March Hare .; March Hare meekly; March\n",
      "Hare took; March Hare .; March Hare said; March Hare ,; March Hare\n",
      "interrupted; March Hare .; March Hare said; March Hare went; March\n",
      "Hare moved; March Hare .; March Hare had; March Hare .; March Hare ,;\n",
      "March Hare .; March Hare said; March Hare interrupted; March Hare .;\n",
      "March Hare and\n"
     ]
    }
   ],
   "source": [
    "my_text.findall('<.*><Hare><.*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`concordance`** searches for a word in a document and displays matches in their original contexts. \n",
    "\n",
    "\n",
    "Let's create a KWIC for lines that include the word _Hare_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 31 matches:\n",
      "aving the other paw , 'lives a March Hare . Visit either you like : they 're b\n",
      " in the direction in which the March Hare was said to live . ' I 've seen hatt\n",
      ", ' she said to herself ; 'the March Hare will be much the most interesting , \n",
      "e in sight of the house of the March Hare : she thought it must be the right h\n",
      "n front of the house , and the March Hare and the Hatter were having tea at it\n",
      "able . 'Have some wine , ' the March Hare said in an encouraging tone . Alice \n",
      "'There is n't any , ' said the March Hare . 'Then it was n't very civil of you\n",
      "out being invited , ' said the March Hare . ' I did n't know it was YOUR table\n",
      " the answer to it ? ' said the March Hare . 'Exactly so , ' said Alice . 'Then\n",
      "ould say what you mean , ' the March Hare went on . ' I do , ' Alice hastily r\n",
      "just as well say , ' added the March Hare , 'that `` I like what I get '' is t\n",
      "e added looking angrily at the March Hare . 'It was the BEST butter , ' the Ma\n",
      "It was the BEST butter , ' the March Hare meekly replied . 'Yes , but some cru\n",
      "n with the bread-knife . ' The March Hare took the watch and looked at it gloo\n",
      "e Hatter . 'Nor I , ' said the March Hare . Alice sighed wearily . ' I think y\n",
      "( ' I only wish it was , ' the March Hare said to itself in a whisper . ) 'Tha\n",
      "ting with his tea spoon at the March Hare , ) ' -- it was at the great concert\n",
      " we change the subject , ' the March Hare interrupted , yawning . ' I 'm getti\n",
      " 'Tell us a story ! ' said the March Hare . 'Yes , please do ! ' pleaded Alice\n",
      " ' 'Take some more tea , ' the March Hare said to Alice , very earnestly . ' I\n",
      "grily , but the Hatter and the March Hare went 'Sh ! sh ! ' and the Dormouse s\n",
      "he Dormouse followed him : the March Hare moved into the Dormouse 's place , a\n",
      "illingly took the place of the March Hare . The Hatter was the only one who go\n",
      "worse off than before , as the March Hare had just upset the milk-jug into his\n",
      " Alice . 'Why not ? ' said the March Hare . Alice was silent . The Dormouse ha\n"
     ]
    }
   ],
   "source": [
    "my_text.concordance('Hare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **concordance** is a listing of all the words in a book or other document. Typically the presentation of concordance lines used here is known as a [keyword-in-context](https://en.wikipedia.org/wiki/Key_Word_in_Context) (KWIC) visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Let's do some more searching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 56 matches:\n",
      "ving its right paw round , 'lives a Hatter : and in THAT direction , ' waving \n",
      " I almost wish I 'd gone to see the Hatter instead ! ' CHAPTER VII . A Mad Tea\n",
      " house , and the March Hare and the Hatter were having tea at it : a Dormouse \n",
      "our hair wants cutting , ' said the Hatter . He had been looking at Alice for \n",
      "severity ; 'it 's very rude . ' The Hatter opened his eyes very wide on hearin\n",
      "t the same thing a bit ! ' said the Hatter . 'You might just as well say that \n",
      "he same thing with you , ' said the Hatter , and here the conversation dropped\n",
      "ng-desks , which was n't much . The Hatter was the first to break the silence \n",
      " . ' 'Two days wrong ! ' sighed the Hatter . ' I told you butter would n't sui\n",
      "bs must have got in as well , ' the Hatter grumbled : 'you should n't have put\n",
      "! ' 'Why should it ? ' muttered the Hatter . 'Does YOUR watch tell you what ye\n",
      "ust the case with MINE , ' said the Hatter . Alice felt dreadfully puzzled . T\n",
      "Alice felt dreadfully puzzled . The Hatter 's remark seemed to have no sort of\n",
      "rmouse is asleep again , ' said the Hatter , and he poured a little hot tea up\n",
      " you guessed the riddle yet ? ' the Hatter said , turning to Alice again . 'No\n",
      "n't the slightest idea , ' said the Hatter . 'Nor I , ' said the March Hare . \n",
      "w Time as well as I do , ' said the Hatter , 'you would n't talk about wasting\n",
      "ice . 'Of course you do n't ! ' the Hatter said , tossing his head contemptuou\n",
      "! that accounts for it , ' said the Hatter . 'He wo n't stand beating . Now , \n",
      "Not at first , perhaps , ' said the Hatter : 'but you could keep it to half-pa\n",
      "ay YOU manage ? ' Alice asked . The Hatter shook his head mournfully . 'Not I \n",
      "ce . 'It goes on , you know , ' the Hatter continued , 'in this way : -- `` Up\n",
      "nished the first verse , ' said the Hatter , 'when the Queen jumped up and baw\n",
      "lice . 'And ever since that , ' the Hatter went on in a mournful tone , 'he wo\n",
      "ed . 'Yes , that 's it , ' said the Hatter with a sigh : 'it 's always tea-tim\n"
     ]
    }
   ],
   "source": [
    "## Your code:\n",
    "# Search for other characters in the text. Where does Hatter show up? What about Alice? \n",
    "# Can you use your Regex knowledge to find uppercase and lowercase 'hatter' references?\n",
    "\n",
    "my_text.concordance('Hatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 396 matches:\n",
      "    CHAPTER I . Down the Rabbit-Hole Alice was beginning to get very tired of s\n",
      "hat is the use of a book , ' thought Alice 'without pictures or conversations ?\n",
      "so VERY remarkable in that ; nor did Alice think it so VERY much out of the way\n",
      "looked at it , and then hurried on , Alice started to her feet , for it flashed\n",
      " hedge . In another moment down went Alice after it , never once considering ho\n",
      "ped suddenly down , so suddenly that Alice had not a moment to think about stop\n",
      "she fell past it . 'Well ! ' thought Alice to herself , 'after such a fall as t\n",
      "own , I think -- ' ( for , you see , Alice had learnt several things of this so\n",
      "tude or Longitude I 've got to ? ' ( Alice had no idea what Latitude was , or L\n",
      " . There was nothing else to do , so Alice soon began talking again . 'Dinah 'l\n",
      "ats eat bats , I wonder ? ' And here Alice began to get rather sleepy , and wen\n",
      "dry leaves , and the fall was over . Alice was not a bit hurt , and she jumped \n",
      " not a moment to be lost : away went Alice like the wind , and was just in time\n",
      " but they were all locked ; and when Alice had been all the way down one side a\n",
      "on it except a tiny golden key , and Alice 's first thought was that it might b\n",
      "and to her great delight it fitted ! Alice opened the door and found that it le\n",
      "ad would go through , ' thought poor Alice , 'it would be of very little use wi\n",
      "ay things had happened lately , that Alice had begun to think that very few thi\n",
      "rtainly was not here before , ' said Alice , ) and round the neck of the bottle\n",
      "ay 'Drink me , ' but the wise little Alice was not going to do THAT in a hurry \n",
      "bottle was NOT marked 'poison , ' so Alice ventured to taste it , and finding i\n",
      "* * 'What a curious feeling ! ' said Alice ; ' I must be shutting up like a tel\n",
      "for it might end , you know , ' said Alice to herself , 'in my going out altoge\n",
      "garden at once ; but , alas for poor Alice ! when she got to the door , she fou\n",
      " no use in crying like that ! ' said Alice to herself , rather sharply ; ' I ad\n"
     ]
    }
   ],
   "source": [
    "my_text.concordance('Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Hatter :; the Hatter instead; the Hatter were; the Hatter .; The\n",
      "Hatter opened; the Hatter .; the Hatter ,; The Hatter was; the Hatter\n",
      ".; the Hatter grumbled; the Hatter .; the Hatter .; The Hatter 's; the\n",
      "Hatter ,; the Hatter said; the Hatter .; the Hatter ,; the Hatter\n",
      "said; the Hatter .; the Hatter :; The Hatter shook; the Hatter\n",
      "continued; the Hatter ,; the Hatter went; the Hatter with; the Hatter\n",
      ":; the Hatter ,; the Hatter :; the Hatter asked; the Hatter and; the\n",
      "Hatter :; The Hatter was; the Hatter ;; the Hatter ,; the Hatter .;\n",
      "the Hatter .; The Hatter looked; the Hatter .; the Hatter .; the\n",
      "Hatter added; a hatter .; the Hatter ,; the Hatter ,; wretched Hatter\n",
      "trembled; the Hatter began; the Hatter replied; the Hatter went; the\n",
      "Hatter .; the Hatter went; the Hatter ,; the Hatter .; miserable\n",
      "Hatter dropped; the Hatter :; the Hatter ,; the Hatter hurriedly; the\n",
      "Hatter was\n"
     ]
    }
   ],
   "source": [
    "my_text.findall('<.*><[Hh]atter><.*>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbit-Hole Alice; do :; of a; thought Alice; conversations ?; mind (;\n",
      "stupid ); making a; suddenly a; did Alice; ' (; it all; natural );\n",
      "TOOK A; , Alice; seen a; either a; or a; down a; went Alice; like a;\n",
      "that Alice; not a; down a; down a; empty :; thought Alice; such a; 'll\n",
      "all; ' (; . ); time ?; see :; ' (; , Alice; not a; over ); to ?; ' (\n",
      "Alice; . ); ' (; at all; word ); Australia ?; ' (; it ? ); ask :; so\n",
      "Alice; ' (; . ); catch a; like a; wonder ?; here Alice; in a; bats ?;\n",
      "bats ?; cats ?; truth :; eat a; bat ?; upon a; . Alice; not a; in a;\n",
      "moment :; was all; not a; lost :; went Alice; turned a; seen :; in a;\n",
      "by a; doors all; were all; when Alice; been all; upon a; , all; except\n",
      "a; and Alice; upon a; was a; high :; ! Alice; into a; than a; rat-hole\n",
      ":; poor Alice; like a; that Alice; rate a; telescopes :; found a; , (;\n",
      "said Alice; , ); was a; was all; little Alice; in a; , all; them :;\n",
      "that a; with a; from a; so Alice; , (; , a; , ); 'What a; said Alice;\n",
      "like a; indeed :; for a; further :; felt a; said Alice; like a; then\n",
      "?; of a; such a; After a; poor Alice; it :; said Alice; , (; it ); in\n",
      "a; poor Alice; on a; table :; it a; said Alice; ate a; way ?; way ?;\n",
      "size :; but Alice; cried Alice (; English ); ' (; off ); dears ?; be\n",
      "a; you :; thought Alice; see :; them a; , (; LOVE ); hall :; Poor\n",
      "Alice; ever :; said Alice; ' a; ' (; this ); on all; was a; pool all;\n",
      "After a; heard a; with a; and a; other :; in a; ' Alice; in a; .\n",
      "Alice; herself all; talking :; night ?; think :; morning ?; feeling a;\n",
      "I ?; over all; at all; I ca; know all; such a; it all; know all; see\n",
      ":; signify :; 'S all; do :; poor Alice; after all; then ?; up :; cried\n",
      "Alice; with a; being all; that ?; rapidly :; WAS a; said Alice; , a;\n",
      "with all; door :; . ( Alice; find a; then a; them a; . ); said Alice;\n",
      "be a; pool a; was :; be a; only a; thought Alice; mouse ?; talk :;\n",
      "began :; pool ?; ' ( Alice; to a; mouse :; such a; ' A; of a; to a; --\n",
      "a; ' ); thought Alice; 's a; ' (; with all; , Alice; . ); again :;\n",
      "chatte ?; gave a; quiver all; cried Alice; in a; me ?; said Alice; in\n",
      "a; tone :; Dinah :; take a; such a; ' Alice; such a; such a; cried\n",
      "Alice; bristling all; such a; cats :; said Alice; in a; dogs ?; so\n",
      "Alice; eagerly :; such a; ! A; and all; I ca; to a; worth a; kills\n",
      "all; cried Alice; in a; quite a; her :; pale (; , Alice; thought ); in\n",
      "a; it :; were a; and a; , a; . Alice; . A; and a; indeed a; and all;\n",
      "again :; had a; after a; to Alice; them all; quite a; this Alice; be\n",
      "a; , all; They all; in a; . Alice; catch a; you all; ready ?; Silence\n",
      "all; with a; politely :; speak ?; him :; WHAT ?; crossly :; find a;\n",
      "Duck :; generally a; or a; find ?; dear ?; to Alice; said Alice; in a;\n",
      "tone :; at all; hide a; smile :; be a; IS a; Caucus-race ?; said\n",
      "Alice; ' (; . ); out a; in a; , (; , ); then all; they all; won ?;\n",
      "without a; for a; forehead (; him ); and all; prizes ?; quite a; to\n",
      "Alice; in a; ' Alice; out a; , (; it ); a-piece all; have a; pocket ?;\n",
      "to Alice; 'Only a; said Alice; they all; they all; . Alice; they all;\n",
      "comfits :; in a; said Alice; in a; is a; and a; to Alice; IS a; said\n",
      "Alice; you call; sad ?; this :; to a; law :; have a; trial :; Such a;\n",
      "Fury :; to Alice; of ?; said Alice; humbly :; think ?; ' A; said\n",
      "Alice; poor Alice; ' Alice; others all; walked a; 'What a; be a; , a;\n",
      "said Alice; question ?; . Alice; pet :; such a; you ca; eat a; caused\n",
      "a; once :; and a; in a; were all; they all; and Alice; in a; poor\n",
      "Alice; In a; heard a; in a; wonder ?; ' Alice; in a; noticed Alice;\n",
      "here ?; me a; and a; And Alice; upon a; was a; ' Alice; for a; happen\n",
      ":; Miss Alice; in a; ' Alice; into a; with a; it (; hoped ) a; gloves\n",
      ":; and a; upon a; such a; expected :; I ca; floor :; as a; me ?; for\n",
      "Alice; larger :; poor Alice; be a; in a; thought Alice; now ?; be a;\n",
      "foolish Alice; here ?; at all; quite a; after a; heard a; came a; .\n",
      "Alice; about a; and Alice; proved a; . Alice; thought Alice; made a;\n",
      "heard a; and a; and a; into a; you ?; then a; ' (; . ); window ?; ' (;\n",
      "' ); size ?; honour :; for all; rate :; was a; and Alice; at all; at\n",
      "all; 'What a; thought Alice; more :; came a; of a; voices all;\n",
      "together :; words :; ladder ?; bear ?; ' ( a; crash ); that ?; chimney\n",
      "?; he ?; said Alice; for a; deal :; kick a; heard a; animal (; was );\n",
      "her :; was a; fellow ?; you ?; us all; came a; , (; thought Alice; ,\n",
      "); 'm a; -- all; like a; like a; and Alice; was a; and Alice; After a;\n",
      "and Alice; ' A; ' A; WHAT ?; thought Alice; moment a; put a; . Alice;\n",
      "were all; and a; it ca; quite a; of a; They all; made a; at Alice; in\n",
      "a; said Alice; , a; in a; said Alice; in a; frightened all; of all; up\n",
      "a; off all; with a; then Alice; behind a; then Alice; having a; with\n",
      "a; began a; running a; and a; hoarsely all; down a; to Alice a; what\n",
      "a; said Alice; against a; leaves :; managed ?; what ?; what ? Alice;\n",
      "looked all; was a; of a; smoking a; from a; and Alice; silence :; in\n",
      "a; YOU ?; for a; . Alice; that ?; I ca; said Alice; I ca; ' Alice; I\n",
      "ca; in a; said Alice; into a; into a; it a; you ?; 'Not a; said Alice;\n",
      "YOU ?; . Alice; felt a; 'Why ?; as Alice; in a; certainly : Alice;\n",
      "that all ?; said Alice; . Alice; after all; you ?; said Alice; I ca;\n",
      "things ?; it all; ' Alice; in a; . Alice; began :; right ?; turned a;\n",
      "that ?; kept all; you a; couple ?; it ?; an eel; clever ?; listen all;\n",
      "stuff ?; said Alice; be ?; ' Alice; . Alice; nothing :; now ?; be a;\n",
      "said Alice :; such a; is a; spoke (; high ); poor Alice; in a; time\n",
      "Alice; In a; WHAT ?; WHAT ?; thought Alice; . Alice; for a; this a;\n",
      "off a; which ?; nibbled a; effect :; felt a; chin :; was a; swallow a;\n",
      "said Alice; in a; found : all; like a; of a; CAN all; be ?; said\n",
      "Alice; to ?; I ca; you ?; except a; like a; into a; when a; in a;\n",
      "hurry : a; NOT a; said Alice; in a; with a; said Alice; ' Alice; had\n",
      "a; said Alice; to a; NOT a; said Alice; 'm a; 'm a; you ?; 'm a; said\n",
      "Alice; ' A; in a; seen a; such a; 're a; said Alice; was a; 're a; 's\n",
      "all; such a; to Alice; for a; 're a; or a; serpent ?; matters a; said\n",
      "Alice; YOURS :; in a; . Alice; After a; in a; puzzling all; size :;\n",
      "wonder ?; with a; thought Alice; size :; For a; suddenly a; -- (; be\n",
      "a; livery :; him a; fish ); with a; like a; , Alice; curled all; was\n",
      "all; crept a; arm a; in a; words a; . Alice; . Alice; such a; was a;\n",
      "-- a; then a; if a; said Alice; in ?; sky all; this Alice; he ca; in\n",
      "?; and a; head :; in ?; asked Alice; in a; at all ?; doubt :; only\n",
      "Alice; way all; this a; do ?; said Alice; said Alice; desperately :;\n",
      "into a; other :; on a; nursing a; stirring a; ' Alice; without a; and\n",
      "a; said Alice; , a; that ?; 's a; that Alice; again :; 'They all; '\n",
      "Alice; into a; 's a; ' Alice; at all; followed a; cried Alice; in a;\n",
      "round a; said Alice; off a; ' Alice; again :; twelve ?; singing a; it\n",
      "a; line :; sneezes :; . (; joined ) :; that Alice; words :; it a; to\n",
      "Alice; threw a; . Alice; was a; in all; like a; thought Alice; like a;\n",
      ", (; into a; , ); thought Alice; in a; two :; behind ?; reply (; time\n",
      "); said Alice; at all a; and Alice; had a; like a; than a; for a; baby\n",
      ":; altogether Alice; at all; into a; said Alice; again (; which ); .\n",
      "Alice; home ?; it :; than a; made a; child :; rather a; was a; on a;\n",
      "of a; tree a; saw Alice; thought :; and a; at all; name :; grinned a;\n",
      "thought Alice; here ?; depends a; said Alice; ' Alice; ' Alice; here\n",
      "?; 'lives a; Hatter :; 'lives a; like :; ' Alice; you ca; Cat :; 're\n",
      "all; mad ?; said Alice; ' Alice; at all; mad ?; ' a; that ?; said\n",
      "Alice; , a; I call; said Alice; to-day ?; said Alice; . Alice; baby ?;\n",
      "into a; ' Alice; in a; . Alice; waited a; after a; on a; of a; fig ?;\n",
      "replied Alice; suddenly :; seen a; without a; thought Alice; 'but a;\n",
      "without a; Hare :; large a; high :; after all; . A; was a; under a; it\n",
      ": a; as a; thought Alice; was a; were all; it :; saw Alice; said\n",
      "Alice; in a; . Alice; looked all; said Alice; said Alice; for a; at\n",
      "Alice; ' Alice; but all; is a; like a; writing-desk ?; thought Alice;\n",
      "it ?; said Alice; ' Alice; thing a; for a; while Alice; over all; it\n",
      "?; to Alice :; . Alice; considered a; grumbled :; gloomily :; again :;\n",
      "' Alice; 'What a; it ?; is ?; ' Alice; readily :; such a; . Alice;\n",
      "poured a; yet ?; to Alice; ' Alice; replied :; answer ?; . Alice; said\n",
      "Alice; ' Alice; replied :; lessons :; whisper a; in a; ' (; in a; . );\n",
      "said Alice; thoughtfully :; Hatter :; manage ?; ' Alice; ' (; , );\n",
      "perhaps ?; said Alice; way :; Like a; exclaimed Alice; in a; do a; '\n",
      "A; into Alice; here ?; with a; sigh :; suppose ?; said Alice; Hatter\n",
      ":; again ?; ' Alice; us a; said Alice; in a; voice :; us a; pleaded\n",
      "Alice; upon a; in a; of a; on ?; said Alice; took a; thinking a; '\n",
      "Alice; ' Alice; on :; of a; well ?; to Alice; ' Alice; I ca; you ca;\n",
      "Hatter :; said Alice; now ?; . Alice; this :; of a; well ?; took a;\n",
      "was a; ' Alice; you ca; ' Alice; draw ?; said Alice; at all; want a;\n",
      "Hatter :; 's all; him :; and Alice; change :; and Alice; was a; .\n",
      "Alice; cautiously :; from ?; of a; of a; stupid ?; ' Alice; poor\n",
      "Alice; drew all; M ?; said Alice; not ?; . Alice; into a; with a; on\n",
      ":; of a; such a; as a; of a; muchness ?; said Alice; than Alice; bear\n",
      ":; would call; her :; said Alice; in all; had a; mushroom (; kept a;\n",
      "pocket ); about a; high :; passage :; Croquet-Ground A; garden :; .\n",
      "Alice; this a; in a; for ?; of all; upon Alice; suddenly :; and all;\n",
      "said Alice; , a; roses ?; in a; been a; put a; should all; was a; and\n",
      "Alice; were all; corners :; ornamented all; couples :; were all; them\n",
      "Alice; Rabbit :; in a; on a; of all; . Alice; to lie; such a; of a;\n",
      "had all; to lie; it ?; to Alice; they all; this ?; to Alice; child ?;\n",
      "is Alice; said Alice; only a; after all; THESE ?; know ?; said Alice;\n",
      "for a; like a; said Alice; dear :; only a; in a; here ?; in a; to\n",
      "Alice; said Alice; into a; for a; off ?; croquet ?; at Alice; shouted\n",
      "Alice; and Alice; 's a; said a; said Alice :; Duchess ?; in a; for ?;\n",
      "said Alice; What a; '' ?; said Alice :; at all a; for ?; . Alice; gave\n",
      "a; in a; in a; in all; in a; . Alice; such a; was all; difficulty\n",
      "Alice; flamingo :; hedgehog a; such a; laughing :; away :; besides\n",
      "all; generally a; , Alice; was a; players all; quarrelling all; in a;\n",
      "in a; in a; . Alice; uneasy :; me ?; noticed a; air :; it a; be a; Cat\n",
      ":; on ?; . Alice; then Alice; at all; ' Alice; rather a; they all; one\n",
      "ca; is all; Queen ?; in a; at all; said Alice :; listening :; to ?; to\n",
      "Alice; 's a; -- a; said Alice :; at all; King :; behind Alice; ' A; at\n",
      "a; said Alice; settling all; . Alice; at all; in a; to Alice; other :;\n",
      "where Alice; in a; into a; sight :; thought Alice; 'as all; for a;\n",
      "quite a; it :; was a; were all; while all; moment Alice; by all; they\n",
      "all; off a; was a; from :; such a; had a; , all; . (; . ) Alice;\n",
      "Duchess :; executioner :; 'You ca; into Alice; . Alice; such a; 'M a;\n",
      ", (; in a; though ); out a; that :; was a; I ca; in a; ' Alice; got a;\n",
      "to Alice; . Alice; her :; upon Alice; conversation a; Duchess :; '\n",
      "Alice; into Alice; ' Alice; after a; pause :; experiment ?; ' Alice;\n",
      "at all; Duchess :; of a; n't a; ' Alice; Duchess :; 'what a; 's a;\n",
      "said Alice; that Alice; 's a; exclaimed Alice; 's a; ' Alice; down :;\n",
      "I ca; in a; said Alice; you a; ' A; thought Alice; again ?; 've a;\n",
      "said Alice; feel a; to Alice; . Alice; like a; ' A; in a; in a; to\n",
      "Alice; and Alice; say a; shade :; that a; . All; and all; and Alice;\n",
      "to Alice; yet ?; said Alice; what a; said Alice; , Alice; in a; are\n",
      "all; 'S a; upon a; . (; what a; . ); leaving Alice; . Alice; Queen :;\n",
      "eyes :; sight :; to Alice; fun ?; said Alice; 's all; that :; thought\n",
      "Alice; it :; in all; on a; , Alice; sorrow ?; 's all; that :; in a;\n",
      "tone :; speak a; . Alice; with a; was a; by a; . Alice; sobbing a; to\n",
      "call; you call; one ?; ' Alice; angrily :; such a; poor Alice; be all;\n",
      "words :; interrupted Alice; before Alice; to a; said Alice; as all;\n",
      "extras ?; Turtle a; said Alice; washing ?; said Alice; n't a; in a;\n",
      "said Alice; with a; that ?; inquired Alice; ' Alice; it ?; suppose ?;\n",
      "said Alice; doubtfully :; ARE a; ' Alice; learn ?; Seaography :; once\n",
      "a; week :; like ?; said Alice; I ca; said :; Gryphon :; with a; sigh\n",
      ":; hours a; lessons ?; said Alice; in a; Turtle :; 'What a; exclaimed\n",
      "Alice; remarked :; quite a; to Alice; over a; been a; holiday ?;\n",
      "twelfth ?; ' Alice; in a; tone :; at Alice; for a; had a; Gryphon :;\n",
      "again :; ' (; said Alice ); to a; ' ( Alice; ' ); what a; thing a;\n",
      "said Alice; of a; it ?; into a; cleared all; with a; as a; said :;\n",
      "with a; 'Turn a; 's all; things all; at Alice; be a; said Alice; see\n",
      "a; it ?; said Alice; sing ?; round Alice; sadly :; walk a; faster ?;\n",
      "said a; to a; 's a; turtles all; dance ?; dance ?; dance ?; gave a; go\n",
      "?; dance ?; dance ?; 's a; said Alice; last :; course ?; said Alice; '\n",
      "Alice; 're all; Turtle :; would all; and all; fall a; 's all; said\n",
      "Alice; about a; called a; whiting ?; said Alice; 'Why ?; . Alice; in\n",
      "a; with ?; shiny ?; ' Alice; considered a; in a; with a; of ?; '\n",
      "Alice; in a; impatiently :; said Alice; please :; said :; without a;\n",
      "really ?; said Alice; in a; Turtle :; if a; going a; porpoise ?; '' ?;\n",
      "said Alice; said Alice a; timidly :; was a; 'Explain all; tone :; such\n",
      "a; So Alice; was a; words all; drew a; 's all; 'It all; over Alice;\n",
      "thought Alice; indeed :; As a; ' [; are all; as a; has a; was a; '\n",
      "Alice; in a; 'She ca; toes ?; know ?; ' Alice; impatiently :; ' Alice;\n",
      "would all; in a; voice :; sharing a; ' [; was all; as a; spoon :; with\n",
      "a; repeating all; on ?; Gryphon :; and Alice; Quadrille ?; you a; song\n",
      "?; , a; ' Alice; in a; fellow ?; in a; this :; in a; stoop ?; the e;\n",
      "-- e; dish ?; give all; Soup ?; Soup ?; the e; -- e; when a; taking\n",
      "Alice; it ?; ' Alice; words :; the e; -- e; Tarts ?; with a; -- all;\n",
      "cards :; with a; with a; and a; was a; with a; it :; made Alice; .\n",
      "Alice; in a; , (; , ); at all; thought Alice; ' (; , ); it :; at all;\n",
      "were all; doing ?; ' Alice; 'They ca; ' Alice; in a; . Alice; that\n",
      "all; ' A; thought Alice; had a; , Alice; juror (; Lizard ); at all;\n",
      "hunting all; follows :; , All; on a; day :; 's a; with a; and a; in :;\n",
      "begin ?; down all; made a; 'm a; at all :; bit a; moment Alice; felt\n",
      "a; her a; was :; I ca; said Alice; meekly :; said Alice; boldly :; at\n",
      "a; Dormouse :; . All; 'm a; in a; above a; what ?; with a; for a;\n",
      "dunce ?; 'm a; in a; King :; too :; say ?; I ca; 'm a; 're a; . (;\n",
      "rather a; had a; strings :; . ); thought Alice; 's all; I ca; Hatter\n",
      ":; thought Alice; officers :; and Alice; sneezing all; in a; with a;\n",
      "in a; of ?; said a; ' Alice; . Alice; cried Alice; such a; upsetting\n",
      "all; of a; in a; had a; in a; 'until all; at Alice; . Alice; in a; had\n",
      "a; out a; , all; business ?; to Alice; said Alice; WHATEVER ?; said\n",
      "Alice; interrupted :; in a; ' Alice; matter a; THAN A; at Alice; not\n",
      "a; said Alice; said Alice :; not a; rule :; said Alice; in a; in a; it\n",
      "?; be a; to ?; at all; n't a; after all :; 's a; handwriting ?; ' (;\n",
      "jury all; . ); . (; jury all; . ); they ca; did :; was a; this :; said\n",
      "Alice; Majesty ?; end :; read :; him :; me a; gone (; true ) :; you ?;\n",
      "They all; been (; fit ); be A; from all; said Alice; , (; n't a; , );\n",
      "jury all; saves a; you ca; you ?; it ?; . (; . ); himself :; said\n",
      "Alice; think ?; . (; . ); with a; was a; 's a; said Alice; said Alice;\n",
      "you ?; said Alice; , (; . ); but a; her :; gave a; , Alice; what a;\n",
      "such a; said Alice; , all; WAS a; certainly :; So Alice; what a;\n",
      "little Alice; and all; after a; dream :; little Alice; and all; and\n",
      "all; change (; knew ); herself a; through all; childhood :; many a;\n",
      "ago :; with all; find a; in all\n"
     ]
    }
   ],
   "source": [
    "my_text.findall('<.*><[Aalice><.*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is this useful?**\n",
    "\n",
    "A powerful use of keyword-in-context displays comes from a recent Boston Globe piece cataloging [words spoken by suspects at or around their arrest](http://apps.bostonglobe.com/graphics/2016/04/arresting-words/).\n",
    "\n",
    "Here is a screenshot of the piece showing mentions of the word \"phone\":\n",
    "\n",
    "![](imgs/arrest_phones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most basic, but still insightful metric we could get from an entire document a count of unique tokens.\n",
    "\n",
    "This gives us a sense of the vocabulary size and repetition of words. \n",
    "\n",
    "Let's make a function to count the number of times each token appears in our document and use it on our word tokens. \n",
    "\n",
    "Sound fun? Great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# input: list of tokens\n",
    "# returns: dict of counts\n",
    "def get_counts(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "\n",
    "# input: dictionary of tokens:counts\n",
    "# returns: sorted list of (token, count)\n",
    "def sort_counts(counts):\n",
    "    return sorted(counts.items(), key=lambda count: count[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get_sorted_counts just runs get_counts and sort_counts together.\n",
    "\n",
    "# input: list of tokens\n",
    "# returns: list of (token, count) values \n",
    "#  sorted with most used counts on top.\n",
    "def get_sorted_counts(tokens):\n",
    "    return sort_counts(get_counts(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# how many counts does Alice have?\n",
    "counts = get_counts(tokens)\n",
    "counts['Alice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also use `get_sorted_counts` to see most used tokens in our document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2418),\n",
       " ('the', 1516),\n",
       " (\"'\", 1311),\n",
       " ('.', 981),\n",
       " ('and', 757),\n",
       " ('to', 717),\n",
       " ('a', 614),\n",
       " ('I', 543),\n",
       " ('it', 513),\n",
       " ('she', 507),\n",
       " ('of', 496),\n",
       " ('said', 456),\n",
       " ('!', 450),\n",
       " ('Alice', 394),\n",
       " ('was', 362),\n",
       " ('in', 351),\n",
       " ('you', 337),\n",
       " ('that', 267),\n",
       " ('--', 264),\n",
       " ('her', 243)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# what are the top used tokens in our data?\n",
    "sorted_counts = get_sorted_counts(tokens)\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Filtering & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "\n",
    "There is a lot of punctuation in that top list. Let's deal with that now. \n",
    "\n",
    "We can create a new function that strips out any tokens that are considered punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# input: string or list of tokens to remove\n",
    "# output: list of tokens with remove_tokens removed\n",
    "def remove_tokens(tokens, remove_tokens):\n",
    "    return [token for token in tokens if token not in remove_tokens]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~--''`\n"
     ]
    }
   ],
   "source": [
    "# we can get a starting point for punctuation from python string\n",
    "from string import punctuation\n",
    "\n",
    "# augmenting the base set - to better fit this data.\n",
    "punc = punctuation + \"--''`\"\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc_tokens = remove_tokens(tokens, punc)\n",
    "no_punc_tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Case\n",
    "\n",
    "You also notice that some of our words start with a capital letter and some don't. We can normalize all our tokens by using a function to convert them all to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# output: list of tokens with every token having only lowercase letters.\n",
    "def lowercase(tokens):\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets combine the two to **normalize** our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1617),\n",
       " ('and', 810),\n",
       " ('to', 720),\n",
       " ('a', 631),\n",
       " ('she', 545),\n",
       " ('i', 543),\n",
       " ('it', 540),\n",
       " ('of', 499),\n",
       " ('said', 462),\n",
       " ('alice', 396),\n",
       " ('was', 367),\n",
       " ('you', 359),\n",
       " ('in', 358),\n",
       " ('that', 284),\n",
       " ('as', 256),\n",
       " ('her', 248),\n",
       " (\"n't\", 217),\n",
       " ('at', 209),\n",
       " (\"'s\", 200),\n",
       " ('on', 192)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "normalized_tokens = remove_tokens(lowercase(tokens), punc)\n",
    "\n",
    "# run sort again\n",
    "sorted_counts = get_sorted_counts(normalized_tokens)\n",
    "\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "We have a list of top words - but they mostly look pretty boring. Of course 'the' is the most used word here - because it is the most used word everywhere! \n",
    "\n",
    "The rationale behind doing this is that typically these words add little value towards extracting meaning from text - as they are used so frequently in normal text. So we can just take them out! \n",
    "\n",
    "But be careful about this filtering process! With this we are again removing data, so it’s good to just pause and make sure this reduction is useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# import stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# check out what they look like.\n",
    "print(len(stops))\n",
    "print(stops[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 462),\n",
       " ('alice', 396),\n",
       " (\"n't\", 217),\n",
       " (\"'s\", 200),\n",
       " ('little', 128),\n",
       " ('one', 99),\n",
       " ('would', 90),\n",
       " ('know', 87),\n",
       " ('could', 86),\n",
       " ('like', 85),\n",
       " ('went', 83),\n",
       " ('queen', 75),\n",
       " ('thought', 74),\n",
       " ('time', 68),\n",
       " ('see', 67),\n",
       " ('king', 63),\n",
       " (\"'m\", 59),\n",
       " ('turtle', 59),\n",
       " ('began', 58),\n",
       " (\"'ll\", 57)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use remove_tokens to remove stop words\n",
    "filtered_normalized_tokens = remove_tokens(normalized_tokens, stops)\n",
    "\n",
    "sorted_counts = get_sorted_counts(filtered_normalized_tokens)\n",
    "\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Data analysis is 80% data cleaning right? So let's keep cleaning!\n",
    "\n",
    "Notice we still have a lot of odd tokens that are the ends of concatenations. \n",
    "\n",
    "Develop a function to remove these tokens from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 462),\n",
       " ('alice', 396),\n",
       " ('little', 128),\n",
       " ('one', 99),\n",
       " ('would', 90),\n",
       " ('know', 87),\n",
       " ('could', 86),\n",
       " ('like', 85),\n",
       " ('went', 83),\n",
       " ('queen', 75),\n",
       " ('thought', 74),\n",
       " ('time', 68),\n",
       " ('see', 67),\n",
       " ('king', 63),\n",
       " ('turtle', 59),\n",
       " ('began', 58),\n",
       " ('hatter', 56),\n",
       " ('mock', 56),\n",
       " ('quite', 55),\n",
       " ('gryphon', 55)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code:\n",
    "# Finish this function to filter these word fragments\n",
    "# Hint: \"'\" not in token  \n",
    "# ^ this will return true if the ' character isn't in the string 'token'\n",
    "def remove_word_fragments(tokens):\n",
    "    return [token for token in tokens if \"'\" not in token]\n",
    "\n",
    "# Then we will call it to further filter our words.\n",
    "more_filtered_normalized_tokens = remove_word_fragments(filtered_normalized_tokens)\n",
    "\n",
    "sorted_counts = get_sorted_counts(more_filtered_normalized_tokens)\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a transformation process that seeks to convert words to their \"base\" or root forms. \n",
    "\n",
    "So, for example, **housing**, **housed**, and **house** could all be collapsed to the root **hous**. \n",
    "\n",
    "The idea is to collapse these similar words into a single token representation in the document. \n",
    "\n",
    "This isn't great for visualizations - but can be useful when counting, comparing, or developing other metrics. \n",
    "\n",
    "Here we use the `PorterStemmer` which implements one of the most popular stemming algorithms for English-language documents. \n",
    "\n",
    "(This algorithm is called the \"Porter Stemmer\" because it was developed by a Dr. [Martin Porter](https://en.wikipedia.org/wiki/Martin_Porter) in 1980.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hous', 'hous', 'hous', 'mous', 'mousi', 'mousey']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "some_words = ['house', 'housing', 'housed', 'mouse', 'mousy', 'mousey']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in some_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting keywords is a very important tasks when working with text. Keywords start to answer the question \"What is this document about\". \n",
    "\n",
    "They can work to summarize a document, providing a starting point for topic analysis. Keywords can also show what makes a document unique.\n",
    "\n",
    "But what are keywords? Keywords (or keyphrases) can be defined as _distinctive_ words or phrases in a document. Obviously, this definition relies on what we mean by distinctive. As you might expect, there are many methods for conjuring up distinctive phrases from text. \n",
    "\n",
    "Here we will look at two approaches:\n",
    "\n",
    "* comparing terms to other terms within the document with collocations.\n",
    "* comparing words to other words from an external corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Collocations\n",
    "\n",
    "A **collocation** is a set of words that occur together more often then chance. These expressions consist of two or more words and can sometimes correspond to some conventional way of saying something.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a handy **Collocation Finder** class that can help us here. \n",
    "\n",
    "We create a new bigram finder by passing in our tokens to the `BigramCollocationFinder.from_words()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to get \"good\" common bigrams out, we need to define a metric to sort bigrams so that interesting collocations can bubble to the top. \n",
    "\n",
    "_So what should this sorting metric be?_\n",
    "\n",
    "NLTK has a few built in ones to choose from, let's look at one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been using word frequency a lot to examine our data processing procedures, what if we just used counts (frequencies) of bigrams? \n",
    "\n",
    "More interesting collocations should appear more often - right?\n",
    "\n",
    "Lets use the `raw_freq` meausure to score bigrams based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('said', 'the'), 0.007674230740985533),\n",
       " (('of', 'the'), 0.004700007343761475),\n",
       " (('said', 'alice'), 0.004259381655283836),\n",
       " (('in', 'a'), 0.0035617243151942423),\n",
       " (('and', 'the'), 0.002900785782477785),\n",
       " (('in', 'the'), 0.002900785782477785),\n",
       " (('it', 'was'), 0.002680472938238966),\n",
       " (('the', 'queen'), 0.00253359770874642),\n",
       " (('to', 'the'), 0.00253359770874642),\n",
       " (('the', 'king'), 0.002276566057134464)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# built in bigram metrics are in here\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "# we call score_ngrams on the finder to produce a sorted list\n",
    "# of bigrams. Each comes with its score from the metric, which\n",
    "# is how they are sorted. \n",
    "finder.score_ngrams(bigram_measures.raw_freq)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? These collocations are **BORING**\n",
    "\n",
    "As you might guess by now, \n",
    "\n",
    "It would be better to use a scoring function that took into account the unusual-ness of the terms into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Log likelihood\n",
    "\n",
    "A better method could use the _likelihood_ of two words occurring together to generate a distance metric. \n",
    "\n",
    "Log likelihood is one such method. Log likelihood looks at the counts of events occuring together vs them occuring separately to try to tease out the difference between _surprise_ and _coincidence_. \n",
    "\n",
    "Here, the events are any two words appearing together. The calculation looks at how often words appear together vs the same words appearing not together to come up with likelihood scores. \n",
    "\n",
    "Fortunately, NLTK makes it super easy to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mock', 'turtle'), 781.0958309926607),\n",
       " (('said', 'the'), 597.9725394724204),\n",
       " (('said', 'alice'), 505.47797818438784),\n",
       " (('i', \"'m\"), 468.5116913414714),\n",
       " (('march', 'hare'), 461.9215891322542),\n",
       " (('went', 'on'), 376.07074397098586),\n",
       " (('do', \"n't\"), 372.7069673845066),\n",
       " (('the', 'queen'), 351.3982433644536),\n",
       " (('the', 'king'), 342.27732696590454),\n",
       " (('in', 'a'), 337.9247961471988),\n",
       " (('the', 'gryphon'), 284.0399154326837),\n",
       " (('the', 'mock'), 277.94479670005336),\n",
       " (('a', 'little'), 276.1051075393118),\n",
       " (('the', 'hatter'), 266.9307751748064),\n",
       " (('ca', \"n't\"), 264.4268785944248),\n",
       " (('you', 'know'), 258.02012533760467),\n",
       " (('white', 'rabbit'), 257.569682778391),\n",
       " (('she', 'had'), 247.48309230433605),\n",
       " (('wo', \"n't\"), 234.68843587296746),\n",
       " (('it', 'was'), 226.63566921399)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.score_ngrams(bigram_measures.likelihood_ratio)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement over raw frequencies in terms of what we would consider 'interesting' phrases.\n",
    "\n",
    "What if we combined this scoring with stop word removal? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Create a Bigram Collocation Finder and run it on the tokens with stop words filtered out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mock', 'turtle'), 702.4708849514692),\n",
       " (('march', 'hare'), 418.4260621097311),\n",
       " (('said', 'alice'), 388.8517255233859),\n",
       " (('white', 'rabbit'), 226.72427008245083),\n",
       " (('ca', \"n't\"), 226.54771413407772),\n",
       " (('wo', \"n't\"), 201.02075776402467),\n",
       " ((\"'of\", 'course'), 137.3065753665882),\n",
       " (('join', 'dance'), 133.59834520253636),\n",
       " ((\"'it\", \"'s\"), 130.64370050380592),\n",
       " (('minute', 'two'), 117.34666875814106)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code\n",
    "# create a BigramCollocationFinder from the stopword filtered tokens\n",
    "# use your choice of scoring metric to score bigrams. \n",
    "# You can use tab completion on the bigram_measures to see other metrics too\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(filtered_normalized_tokens)\n",
    "finder.score_ngrams(bigram_measures.likelihood_ratio)[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('said', 'alice'), 0.009103693286951374),\n",
       " (('mock', 'turtle'), 0.004144770927392495),\n",
       " (('march', 'hare'), 0.0022944267633779884),\n",
       " (('said', 'king'), 0.0021463992302568277),\n",
       " (('ca', \"n't\"), 0.0019983716971356674),\n",
       " (('thought', 'alice'), 0.001924357930575087),\n",
       " ((\"'it\", \"'s\"), 0.0017763303974539263),\n",
       " (('wo', \"n't\"), 0.0017763303974539263),\n",
       " (('said', 'hatter'), 0.0016283028643327658),\n",
       " (('white', 'rabbit'), 0.0016283028643327658)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.score_ngrams(bigram_measures.raw_freq)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('said', 'alice'), 0.009103693286951374),\n",
       " (('mock', 'turtle'), 0.004144770927392495),\n",
       " (('march', 'hare'), 0.0022944267633779884),\n",
       " (('said', 'king'), 0.0021463992302568277),\n",
       " (('ca', \"n't\"), 0.0019983716971356674),\n",
       " (('thought', 'alice'), 0.001924357930575087),\n",
       " ((\"'it\", \"'s\"), 0.0017763303974539263),\n",
       " (('wo', \"n't\"), 0.0017763303974539263),\n",
       " (('said', 'hatter'), 0.0016283028643327658),\n",
       " (('white', 'rabbit'), 0.0016283028643327658)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.score_ngrams(bigram_measures.raw_freq)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading**\n",
    "\n",
    "* [Surprise and Coincidence](http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html)\n",
    "* [Stereotropes likelihood](http://stereotropes.bocoup.com/about#gender-association)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to the English Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "\n",
    "We have seen that word counts, also known as **term frequency** is not a very useful metric when attempting to find interesting words or phrases. Even with stopwords removed, the most frequent terms in a document are typically pretty boring. \n",
    "\n",
    "The core problem really is that not all words in a document carry the same weight in terms of significance or pertinence. \n",
    "\n",
    "The words \"the\" and \"turtle\" do not provide the same amount of information. \"the\" is a word used all the time in english. If a document has a bunch of \"the\"s in it, that really doesn't tell us anything. But, if you see lots of \"turtle\"s, you might want to pay attention.\n",
    " \n",
    "\n",
    "TF-IDF tries to capture this idea: _words that don’t occur in often in communication but which occur a lot in your document are important to the document’s content._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it would be nice to know the frequency of use of all words in all communications in a language, practically speaking - that is impossible.\n",
    "\n",
    "So a **corpus** or collection of documents is typically used as a proxy to what general communcation looks like. \n",
    "\n",
    "We compare our document against a collection of documents to find out what words or phrases make our document unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Calculation\n",
    "\n",
    "But what is TF-IDF exactly, and how do we calculate it? Let's get to that now!\n",
    "\n",
    "We know **term frequency** is just the count of a particular token or term in a document. \n",
    "\n",
    "**document frequency** is defined as the count of **documents** that a particular token appears in. \n",
    "\n",
    "\n",
    "_Quick Example:_\n",
    "\n",
    "\n",
    "Say we have 3 documents in our corpus, and the token we are looking for is \"turtle\". \n",
    "\n",
    "* Doc 1 has turtle 3 times\n",
    "* Doc 2 has turtle 0 times\n",
    "* Doc 3 has turtle 12 times\n",
    "\n",
    "Those counts right there are the term frequencies for the particular term in particular documents. \n",
    "\n",
    "The document frequency of \"turtle\" would be _2_ - as two of the three of the documents in the corpus contain turtle. \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "To get the **inverse document frequency**, we simply take \n",
    "\n",
    "```\n",
    "1 / document frequency\n",
    "```\n",
    "\n",
    "So the full calculation is just:\n",
    "\n",
    "```\n",
    "term frequency * (1 / document frequency)\n",
    "```\n",
    "\n",
    "Seems too simple to be true - and it is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The calculation for **IDF** we will use is:\n",
    "\n",
    "```\n",
    "log(1 + N / document frequency)\n",
    "```\n",
    "\n",
    "Where `N` is the number of documents in the corpus.\n",
    "\n",
    "\n",
    "I said the above calculation was too easy, and it is. There are a few optimizations to IDF that are typically applied.\n",
    "\n",
    "* We want to make sure we aren't dividing by zero.\n",
    "\n",
    "* The linear weighting is a bit heavy handed. A rare token found in two documents is most likely not half as interesting as a token found in just one document. \n",
    "\n",
    "* We probably want to normalize based on corpus size.\n",
    "\n",
    "\n",
    "Here are some great graphs from a [good TF-IDF explaination](https://porganized.com/2016/03/09/term-weighting-for-humanists/) that show how different IDF calculations look as a function of document frequency. \n",
    "\n",
    "![](imgs/idf-curve1.png)\n",
    "\n",
    "For the term frequency calculation, **TF**, we will also divide by the number of tokens in the document. \n",
    "\n",
    "This will make it so that longer documents are not unfairly boosted by their token counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Code\n",
    "\n",
    "Enough talking, let's get to coding!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the **brown** corpus, which is a set of documents broken up into differnet categories. \n",
    "\n",
    "(and one of the first [computer readable corpula](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the brown corpus package to our notebook\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Here we can see the categories used to splitup the articles in the dataset.\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate TF-IDF we will need to calculate the document frequency for any token. This means we should keep each document in our corpus separate (as opposed to mashing it all together in one big bag of words) so we know if a token occurs in a particular document. \n",
    "\n",
    "Here is one way I found to keep the tokens for each document in the brown corpus in a separate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# our corpus tokens will be an list of lists.\n",
    "news_tokens = []\n",
    "\n",
    "# Iterate over the filenames for each document in brown. \n",
    "# We are using only those documents in the 'news' category to speed things up.\n",
    "for filename in brown.fileids(categories='news'):\n",
    "    # Do our processing on the corpus documents to keep things consistent. \n",
    "    bwords = brown.words(fileids=filename)\n",
    "    processed_news = lowercase(bwords)\n",
    "    processed_news = remove_tokens(processed_news, punc)\n",
    "    processed_news = remove_tokens(processed_news, stops)\n",
    "\n",
    "    news_tokens.append(processed_news)\n",
    "\n",
    "# this is the number of documents in our corpus\n",
    "len(news_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create functions for each portion of the TF-IDF calculation. \n",
    "\n",
    "Lets start with **term frequency**. Assuming we have a dictionary of counts for each token in a document, the calculation becomes simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# returns: dict of counts\n",
    "def get_counts(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# input token: the token we are looking at\n",
    "# input counts: token count dictionary for one document\n",
    "def term_frequency(token, counts):\n",
    "    '''Calculate term frequency for a particular token in a particular document'''\n",
    "    return counts[token] / float(len(counts.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function a bit with our existing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4812698412698413\n",
      "0.12507936507936507\n",
      "0.00031746031746031746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = get_counts(tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(term_frequency('the', counts))\n",
    "print(term_frequency('Alice', counts))\n",
    "print(term_frequency('walrus', counts))\n",
    "\n",
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, let's make a `document_frequency` function. \n",
    "\n",
    "Again, Document Frequency refers to the number of documents that contain a particular token. \n",
    "\n",
    "We will provide a token and our list of lists that stores our corpus. We want out how many documents in that corpus contain the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input token: a token to search the corpora for\n",
    "# input corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "# output: number of documents in corpora that contain the token.\n",
    "def document_frequency(token, corpus_tokens):\n",
    "    '''Returns number of times a token appears in a set of documents'''\n",
    "    doc_count = 0\n",
    "    for tokens in corpus_tokens:\n",
    "        \n",
    "        if token in tokens:\n",
    "            doc_count += 1\n",
    "            \n",
    "    return doc_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 7, 3, 2, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing document_frequency on some bizness and not so bizness words. \n",
    "test_words = [\"business\", \"account\", \"welcome\", \"alice\", \"stillsuit\"]\n",
    "[document_frequency(token, news_tokens) for token in test_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add our optimizations to this metric by creating a `inverse_doc_frequency` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# input: token: token we are analyzing \n",
    "# input: corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "def inverse_doc_frequency(token, corpus_tokens):\n",
    "    return math.log(1 +  len(corpus_tokens) / (document_frequency(token, corpus_tokens) + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n",
      "28\n",
      "0.923163611161917\n",
      "house\n",
      "29\n",
      "0.9028677115420144\n",
      "alice\n",
      "2\n",
      "2.751535313041949\n",
      "hatter\n",
      "0\n",
      "3.8066624897703196\n"
     ]
    }
   ],
   "source": [
    "for token in [\"world\", \"house\", \"alice\", \"hatter\"]:\n",
    "    print(token)\n",
    "    print(document_frequency(token, news_tokens))\n",
    "    print(inverse_doc_frequency(token, news_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And now the big finish!**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input document_tokens: a list of tokens that represent a document\n",
    "# input corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "# output: list of (token, tf-idf) values for each unique token in document_tokens\n",
    "def tf_idf(document_tokens, corpus_tokens):\n",
    "\n",
    "    \n",
    "    # Get our token frequencies for all the unique tokens in our document\n",
    "    token_counts = get_counts(document_tokens)\n",
    "    \n",
    "    # iterate through these tokens and calculate the tf-idf\n",
    "    tfidfs = {}\n",
    "    for token in token_counts.keys():\n",
    "        \n",
    "        tf = term_frequency(token, token_counts)\n",
    "        idf = inverse_doc_frequency(token, corpus_tokens)\n",
    "        \n",
    "        tfidfs[token] = tf * idf\n",
    "        \n",
    "    \n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ~~~WARNING~~~\n",
    "# this takes a while to run!\n",
    "token_tf_idfs = tf_idf(filtered_normalized_tokens, news_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alice', 0.4099352836586199),\n",
       " (\"n't\", 0.31077718595942794),\n",
       " (\"'s\", 0.2864305861377216),\n",
       " ('said', 0.12895976420804076),\n",
       " ('queen', 0.08847331309055163),\n",
       " (\"'m\", 0.08449702291062786),\n",
       " ('turtle', 0.08449702291062786),\n",
       " (\"'ll\", 0.08163271704925065),\n",
       " (\"'and\", 0.08020056411856204),\n",
       " ('hatter', 0.08020056411856204),\n",
       " ('mock', 0.08020056411856204),\n",
       " (\"'it\", 0.07876841118787344),\n",
       " ('gryphon', 0.07876841118787344),\n",
       " (\"'you\", 0.073039799465119),\n",
       " (\"'ve\", 0.06301472895029875),\n",
       " ('duchess', 0.06015042308892153),\n",
       " ('dormouse', 0.057286117227544314),\n",
       " (\"'but\", 0.055853964296855706),\n",
       " ('rabbit', 0.055443276203412356),\n",
       " (\"'re\", 0.0544218113661671)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_token_tf_idfs = sort_counts(token_tf_idfs)\n",
    "sorted_token_tf_idfs[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Summarization\n",
    "\n",
    "**Summarization** is the idea of collapsing a document down to a quick digestable chunk or summary. \n",
    "\n",
    "\n",
    "This is especially interesting for more newsy and technical documents where an \"abstract-like\" summary could be enough for a reader to decide if it is worth reading the document in full. \n",
    "\n",
    "A [simple but interesting algorithm]((http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf) was described in 1957.\n",
    "\n",
    "Check out this [great interactive explaination](http://www.fastforwardlabs.com/luhn/) of this algorithm.\n",
    "\n",
    "Here is the basic idea:\n",
    "\n",
    "* Take a document and remove stop words.\n",
    "* Pick the top X most frequent words, where X is 5 or so.\n",
    "* Rank sentences in the document based on how many times these most frequent words appear in the sentence.\n",
    "* Take the top 3 or 4 sentences as the summary. \n",
    "\n",
    "Most of these pieces we have already, we just need to put them together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading:**\n",
    "\n",
    "* [Intro to Keyphrase Extraction](http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/)\n",
    "* [Term Weighting for Humanists](https://porganized.com/2016/03/09/term-weighting-for-humanists/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
